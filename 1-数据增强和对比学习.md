# 数据增强和对比学习

## 1. 数据增强

**数据增强**(Data Augmentation)，是一种通过对训练数据进行**变换**(transform)来生成新样本的技术。其目标是通过增加数据的多样性，提供更多信息给模型，从而提高模型的泛化能力。

根据操作对象的不同，大致分为三类：

- 序列数据，如文字序列
  - 添加噪声
  - 数据截取
  - 随机交换、删除等
- 图像数据
  - 平移、旋转、缩放、裁剪等
  - 添加噪声、mask等
  - 颜色扰动
- 图结构
  - 节点或边特征向量的随机mask
  - 节点或边的随机删除、添加等
  - 随机采样子图



## 2. 对比学习

> **有监督学习**(Supervised Learning)，是一种从有标注数据中学习预测模型的机器学习方法，常用于回归、分类等任务。例如，给你一堆手写数字（范围0-9）的图片数据集，每张图片都有对应的真实数字标签，通过每条样本数据以及对应标签，你预测未知样本的数字结果。
>
> **无监督学习**(Unsupervised Learning)，是一种从无标注数据中学习预测模型的机器学习方法，用于学习数据中的统计规律和潜在结构，如聚类算法。例如，给你一堆手写数字（范围0-9）的图片数据集，但是没有任何标注信息，让你挖掘给你的数据中有什么统计规律。
>
> **自监督学习**(Self-supervised Learning)，是一种从无标注数据中挖掘出自身监督信息来进行监督学习和训练的机器学习方法。自监督学习虽然需要标注数据，但不用人工提供，而是来源数据本身。主要用于在未标注数据上做预训练，然后迁移到有标注数据的下游任务上。

**对比学习**(Contrastive Learning)，是一种特殊的无监督学习方法（自监督学习），通过让模型将同一样本的不同表示（正样本对）跟接近，不同样本的表示更**分散**（不只是远离，致力于不同样本趋向于均匀分布），使其学到更有用的特征表示，从而更好的应用在有标签的下游任务中。

### 2.1 对比学习方法

如下图所示，原始数据$D$通过多种数据增强方法$\Gamma(·)$，得到数据增强的结果$\tilde{D}$，将$\tilde{D}$送到一个Encoder中学习特征。这个Encoder可以是任意一个深度学习模型，如GNN、GCN。我们将学习到的特征用$f_\theta(\tilde D)$，再将这个特征通过一个投影头(Head projection)$g_{\phi_s}(·)$用来做对比学习任务，一般为一层MLP层，最后将多个数据增强的特征计算对比损失loss，使其让同一样本增强后的$\tilde D$相近，不同样本之间分散。![img](img/v2-518973b87cb9349107ae6bf59bb4318c_r.jpg)

将上述流程公式化表达为：
$$
f_{\theta}^{\ast}=arg\min_{f_{\theta},g_{\phi_s}} \mathcal{L}_{ssl}(g_{\phi_s}(f_{\theta}(\tilde D_1),f_{\theta}(\tilde D_2)))
$$

## 3. 数据增强和对比学习的关系

数据增强方法是对比学习中的一个至关重要的前置操作，即生成更多的样本对。通过对输入数据进行多种增强，可以创建多个视图(view)，作为对比学习中的样本对。同一个样本生成的不同表示作为正样本对，不同样本生成的表示作为负样本对。

$ df $

$`df`$


$\sqrt{3x-1}+(1+x)^2$
$`\sqrt{3x-1}+(1+x)^2`$

```math
\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)
```

$$\left( \sum_{k=1}^n a_k b_k \right)^2 \leq \left( \sum_{k=1}^n a_k^2 \right) \left( \sum_{k=1}^n b_k^2 \right)$$