论文阅读：（MIRACLE）《Towards Multi-Interest Pre-training with Sparse Capsule Network》

> 论文信息
>
> 标题：Towards Multi-Interest Pre-training with Sparse Capsule Network
>
> 来源：SIGIR '23
>
> 地址：https://dl.acm.org/doi/pdf/10.1145/3539618.3591778
>
> 代码：https://github.com/WHUIR/Miracle
>
> tag：Sequential Recommendation; Multi-Interest Learning; Capsule Network; Contrastive Learning; Pre-training

# 1. 背景

**动机**：大部分传统方法（包括预训练）都是为每个用户学习到一个静态、固定的表征作为用户的偏好，这就无法捕捉用户多样的兴趣。因此当预训练一个推荐模型时，考虑用户的多兴趣特征是值得关注的。



**创新点**

本文使用稀疏胶囊网络（由兴趣胶囊激活机制和位置感知的动态路由算法两部分构成）能够自适应地抽取用户兴趣种类并用于后续推荐。

- 不需要预定义用户的兴趣数量
- 在动态路由中显示建模时序信息

# 2. 方法



![image-20231208155935953](../img/image-20231208155935953.png)



## 2.1 文本感知的商品嵌入

> 这部分参考如下文章：Hou Y, Mu S, Zhao W X, et al. Towards universal sequence representation learning for recommender systems[C]//Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining. 2022: 585-593.
>
> domain bias: 当采用多个领域的数据集用作预训练时，由于不同领域之间的语义差距，学到的表征并没有映射到相同的语义空间

- Pre-trained Language Model-based Text Encoder

  - BERT~base~ with frozen parameters to encode the textual information of all items

- MoE Adaptor

  - mixture-of-expert (MoE) network to alleviate the domain bias
    $$
    x_i=\sum\limits_{k=1}^Gg_k\cdot Expert_k(t_i), \\
    g=sofmax(t_i\cdot W_1+b_1),\\
    Expert_k(t_i)=(t_i-b_2^k)\cdot W_2^k
    $$
    其中，$g_k$ 为第 $k$ 个专家网络的门控权值，$Expert_k(·)$ 为第 $k$ 个专家网络，$W_1\in\mathbb{R}^{d_b\times G},b_1\in\mathbb{R}^G,W_2^k\in\mathbb{R}^{d_b\times d_n},b_2^k\in\mathbb{R}^{d_n}$ ，$d_n$ 为专家网络隐藏层大小 ，$G$ 为专家数量

- Deeply-contextualized Item Embedding

  - L-layer Transformer to model the long-short term dependency between items

    - 额外添加可训练的位置嵌入 $P \in \mathbb R^{N\times d_n}$ ；MHA(·) 为多头自注意力

    $$
    F^{0}=[x_{1}+P_{1},x_{2}+P_{2},..,x_{N}+P_{N}], \\
    F^l=FFN(MHA(F^{l-1})), \\
    H = F^L = [h_1,...,h_N]
    $$

## 2.2 Sparse Interest Capsule Network

使用稀疏兴趣胶囊网络抽取用户的多个兴趣。在其动态路由阶段，采用稀疏兴趣胶囊激活机制灵活地确定兴趣数量并初始化耦合系数。

**Sparse Interest Capsule Activation**

- $A \in \mathbb R^{C \times d_n}$ ：基向量矩阵，指代通用兴趣空间，$C$ 为兴趣数量

被激活的胶囊子集反映了用户的多个兴趣。因此可以简单计算商品和基向量的相似度：
$$
s_{ij} = sim(h_i,a_j)·(1+\epsilon)
$$
其中，$sim(·)$ 为内积，$h_i$ 为第 $i$ 个商品的嵌入，$a_j$ 为第 $j$ 个基向量，$\epsilon$ 为高斯噪声。

为每个商品激活一个兴趣胶囊，并生成相应的胶囊激活向量 $m_i$ :
$$
m_{ij}=\begin{cases}-\infty,&\text{if j not in any}\arg\max(s_i);\\0,&\text{otherwise}\end{cases},
$$
其中，$m_{ij}=0$ 意味着第 $j$ 个兴趣胶囊被激活，且 $s_i=[s_{i2},...,s_{iC}]$ 。

> 这意味着对于一个商品，只有一个 m~ij~ 为0，其余全为负无穷

**Position-wise Capsule Network**

考虑位置信息，使用注意力机制计算在不同位置上商品的重要性
$$
o=softmax(tanh((H+\tilde{P})W_3+b_3)W_4+b_4),
$$
其中，$\tilde P \in \mathbb R^{N \times d_n}$ 为另一个适用于多兴趣学习的可训练位置嵌入。

**Multi-Interest Extractor Layer**

使用全连接层，还包括残差连接、层正则化，获得潜兴趣特征：
$$
q_i=LayerNorm(tanh(h_iW_5+b_5)+h_i),
$$
通过下式计算第 $j$ 个胶囊的输出：
$$
z_j=\sum_{i=1}^Nc_{ij}o_i\boldsymbol{q}_i,
$$
其中，$c_{ij}$ 是第 i 个商品和第 j 个胶囊的耦合系数，$o_i$ 是 $o$ 的第 i 个元素。由于每个历史商品只激活一个兴趣胶囊，因此我们在这些激活的胶囊上限制动态路由的范围：
$$
c_{ij}=\frac{\exp(b_{ij}+m_{ij})}{\sum_{k=1}^{C}\exp(b_{ik}+m_{ik})}, \\
u_j=\frac{||z_j||^2}{||z_j||^2+1}\frac{z_j}{||z_j||},\\
b_{ij}=b_{ij}+h_{i}^{T}u_{j}
$$

> 换句话说，在激活胶囊 j 上的 c~ij~ 为 1，其余胶囊上为0

**模型训练**

- Interest-Level Pre-training

  - 对比学习——随机MASK序列元素进行数据增强
    $$
    \ell_i=\sum_{i=1}^{B_m}-log\frac{\exp(\tilde{u}_i^T\tilde{u}_i^{\prime})}{\sum_{j=1}^{B_m}\exp(\tilde{u}_i^T\tilde{u}_j^{\prime})},
    $$

- Predicting Next Item

  - 交叉熵损失
    $$
    \ell_{rec}=\sum_{i=1}^{B}-log\frac{\exp(\hat{y}_{u_{i},v_{i}})}{\sum_{j=1}^{B}\exp(\hat{y}_{u_{i},v_{j}})}, \\
    \hat{y}_{u_{i},v_{j}}=\max(u_{i}^{T}x_{j}|i\in I_{u_{i}})
    $$

- Capsule Regularization

  - 避免所有胶囊都是相同的表达含义——基向量 A 彼此正交
    $$
    \ell_c=\sum_{j=1}^C-log\frac{\cos(a_j,a_j)}{\sum_{j'=1}^C\cos(a_j,a_{j'})},
    $$

  - 尽量使激活的胶囊呈均匀分布，设计如下损失：
    $$
    \mathbf{w}=\frac{1}{B}\sum_{i\in B}s_i, \\
    \mathbf{r}=\frac{1}{B}\sum_{i=1}^B m_i, \\
    \ell_{b}=C\cdot\mathbf{w}^{T}\mathbf{r}
    $$

- 预训练的优化目标：

$$
\ell_{pre-train}=\ell_{rec}+\theta_{1}\ell_{b}+\theta_{2}\ell_{c}+\theta_{3}\ell_{i},
$$

# 思考

推荐+胶囊网络 = 基于用户多兴趣的推荐

- 用户兴趣个数 = 胶囊网络最后一层的个数

文中认为一个商品只属于其中一个兴趣种类更好，而不是与多个兴趣种类都有关联，这种方法是最好的吗？

在拿到用户多个兴趣表征后，融合多兴趣表征是否更好？没有融合的话如果激活胶囊数过多在计算过程中时间复杂度是否过高？

兴趣基向量要彼此正交，能否考虑更好的优化方法？